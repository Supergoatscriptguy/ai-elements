export interface AIModel {
  id: string;
  name: string;
  provider: string;
  inputPrice: number; // per 1K tokens
  outputPrice: number; // per 1K tokens
  contextWindow: number;
  features: string[];
  speed: 'fast' | 'medium' | 'slow';
  category: 'chat' | 'coding' | 'reasoning' | 'multimodal' | 'embedding';
  description?: string;
  popular?: boolean;
  cacheRead?: number;
  cacheWrite?: number;
}

export const allModels: AIModel[] = [
  // OpenAI Models
  {
    id: 'openai/gpt-5',
    name: 'GPT-5',
    provider: 'OpenAI',
    inputPrice: 0.00125,
    outputPrice: 0.01,
    contextWindow: 400000,
    cacheRead: 0.00013,
    features: ['Complex reasoning', 'Code generation', 'Multi-step tasks'],
    speed: 'medium',
    category: 'reasoning',
    description: 'OpenAI\'s flagship language model that excels at complex reasoning, broad real-world knowledge, code-intensive, and multi-step agentic tasks.',
    popular: true,
  },
  {
    id: 'openai/gpt-5-mini',
    name: 'GPT-5 Mini',
    provider: 'OpenAI',
    inputPrice: 0.00025,
    outputPrice: 0.002,
    contextWindow: 400000,
    cacheRead: 0.00003,
    features: ['Cost optimized', 'Fast responses', 'Balanced performance'],
    speed: 'fast',
    category: 'chat',
    description: 'Cost optimized model that excels at reasoning/chat tasks. Offers an optimal balance between speed, cost, and capability.',
    popular: true,
  },
  {
    id: 'openai/gpt-5-nano',
    name: 'GPT-5 Nano',
    provider: 'OpenAI',
    inputPrice: 0.00005,
    outputPrice: 0.0004,
    contextWindow: 400000,
    cacheRead: 0.00001,
    features: ['High throughput', 'Ultra-fast', 'Classification'],
    speed: 'fast',
    category: 'chat',
    description: 'High throughput model that excels at simple instruction or classification tasks.',
  },
  {
    id: 'openai/gpt-4.1',
    name: 'GPT-4.1',
    provider: 'OpenAI',
    inputPrice: 0.002,
    outputPrice: 0.008,
    contextWindow: 1000000,
    cacheRead: 0.0005,
    features: ['Problem solving', 'Large context', 'Complex tasks'],
    speed: 'medium',
    category: 'reasoning',
    description: 'OpenAI\'s flagship model for complex tasks. Well suited for problem solving across domains.',
    popular: true,
  },
  {
    id: 'openai/gpt-4.1-mini',
    name: 'GPT-4.1 Mini',
    provider: 'OpenAI',
    inputPrice: 0.0004,
    outputPrice: 0.0016,
    contextWindow: 1000000,
    cacheRead: 0.0001,
    features: ['Balanced performance', 'Speed', 'Cost effective'],
    speed: 'fast',
    category: 'chat',
    description: 'Provides a balance between intelligence, speed, and cost that makes it attractive for many use cases.',
  },
  {
    id: 'openai/gpt-4.1-nano',
    name: 'GPT-4.1 Nano',
    provider: 'OpenAI',
    inputPrice: 0.0001,
    outputPrice: 0.0004,
    contextWindow: 1000000,
    cacheRead: 0.00003,
    features: ['Ultra-fast', 'Cost effective', 'Lightweight'],
    speed: 'fast',
    category: 'chat',
    description: 'The fastest, most cost-effective GPT 4.1 model.',
  },
  {
    id: 'openai/gpt-4o',
    name: 'GPT-4o',
    provider: 'OpenAI',
    inputPrice: 0.0025,
    outputPrice: 0.01,
    contextWindow: 128000,
    cacheRead: 0.00125,
    features: ['Multimodal', 'General knowledge', 'Complex instructions'],
    speed: 'medium',
    category: 'multimodal',
    description: 'Has broad general knowledge and domain expertise, follows complex instructions, and solves difficult problems accurately. Matches GPT-4 Turbo performance with faster and cheaper API.',
    popular: true,
  },
  {
    id: 'openai/gpt-4o-mini',
    name: 'GPT-4o Mini',
    provider: 'OpenAI',
    inputPrice: 0.00015,
    outputPrice: 0.0006,
    contextWindow: 128000,
    cacheRead: 0.00007,
    features: ['Multimodal', 'Cost efficient', 'Fast responses'],
    speed: 'fast',
    category: 'multimodal',
    description: 'Most advanced and cost-efficient small model. Multi-modal (text or image inputs) with higher intelligence than GPT-3.5 Turbo but just as fast.',
    popular: true,
  },
  {
    id: 'openai/gpt-4-turbo',
    name: 'GPT-4 Turbo',
    provider: 'OpenAI',
    inputPrice: 0.01,
    outputPrice: 0.03,
    contextWindow: 128000,
    features: ['General knowledge', 'Domain expertise', 'Complex instructions'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Has broad general knowledge and domain expertise for complex instructions and difficult problems. Knowledge cutoff: April 2023.',
  },
  {
    id: 'openai/gpt-3.5-turbo',
    name: 'GPT-3.5 Turbo',
    provider: 'OpenAI',
    inputPrice: 0.0005,
    outputPrice: 0.0015,
    contextWindow: 16000,
    features: ['Cost effective', 'Chat optimized', 'Fast responses'],
    speed: 'fast',
    category: 'chat',
    description: 'Most capable and cost effective model in the GPT-3.5 family, optimized for chat but works well for traditional completions.',
  },
  {
    id: 'openai/gpt-3.5-turbo-instruct',
    name: 'GPT-3.5 Turbo Instruct',
    provider: 'OpenAI',
    inputPrice: 0.0015,
    outputPrice: 0.002,
    contextWindow: 8000,
    features: ['Legacy compatible', 'Completions API', 'GPT-3 era'],
    speed: 'fast',
    category: 'chat',
    description: 'Similar capabilities as GPT-3 era models. Compatible with legacy Completions endpoint, not Chat Completions.',
  },
  {
    id: 'openai/o4-mini',
    name: 'O4 Mini',
    provider: 'OpenAI',
    inputPrice: 0.0011,
    outputPrice: 0.0044,
    contextWindow: 200000,
    cacheRead: 0.00028,
    features: ['Fast reasoning', 'Math', 'Coding', 'Visual tasks'],
    speed: 'fast',
    category: 'reasoning',
    description: 'Delivers fast, cost-efficient reasoning with exceptional performance for its size, particularly excelling in math, coding, and visual tasks.',
    popular: true,
  },
  {
    id: 'openai/o3',
    name: 'O3',
    provider: 'OpenAI',
    inputPrice: 0.002,
    outputPrice: 0.008,
    contextWindow: 200000,
    cacheRead: 0.0005,
    features: ['Advanced reasoning', 'Coding', 'Math', 'Science', 'Visual perception'],
    speed: 'slow',
    category: 'reasoning',
    description: 'Most powerful reasoning model, setting new benchmarks in coding, math, science, and visual perception. Excels at complex queries requiring multi-faceted analysis.',
    popular: true,
  },
  {
    id: 'openai/o3-mini',
    name: 'O3 Mini',
    provider: 'OpenAI',
    inputPrice: 0.0011,
    outputPrice: 0.0044,
    contextWindow: 200000,
    cacheRead: 0.00055,
    features: ['Small reasoning', 'High intelligence', 'Cost effective'],
    speed: 'fast',
    category: 'reasoning',
    description: 'Most recent small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini.',
  },
  {
    id: 'openai/o1',
    name: 'O1',
    provider: 'OpenAI',
    inputPrice: 0.015,
    outputPrice: 0.06,
    contextWindow: 200000,
    cacheRead: 0.0075,
    features: ['Deep thinking', 'Multi-step tasks', 'Complex problems'],
    speed: 'slow',
    category: 'reasoning',
    description: 'Flagship reasoning model designed for complex problems requiring deep thinking. Strong reasoning with improved accuracy for multi-step tasks.',
  },
  {
    id: 'openai/gpt-oss-120b',
    name: 'GPT-OSS 120B',
    provider: 'OpenAI',
    inputPrice: 0.0001,
    outputPrice: 0.0005,
    contextWindow: 131000,
    features: ['Open source', 'Large scale', 'Controllable reasoning'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Extremely capable general-purpose LLM with strong, controllable reasoning capabilities.',
  },
  {
    id: 'openai/gpt-oss-20b',
    name: 'GPT-OSS 20B',
    provider: 'OpenAI',
    inputPrice: 0.00007,
    outputPrice: 0.0003,
    contextWindow: 128000,
    features: ['Compact', 'Low latency', 'Edge deployment'],
    speed: 'fast',
    category: 'chat',
    description: 'Compact, open-weight model optimized for low-latency and resource-constrained environments, including local and edge deployments.',
  },
  {
    id: 'openai/text-embedding-3-small',
    name: 'Text Embedding 3 Small',
    provider: 'OpenAI',
    inputPrice: 0.00002,
    outputPrice: 0,
    contextWindow: 8191,
    features: ['Text embeddings', 'Improved performance', 'Small size'],
    speed: 'fast',
    category: 'embedding',
    description: 'OpenAI\'s improved, more performant version of their ada embedding model.',
  },
  {
    id: 'openai/text-embedding-3-large',
    name: 'Text Embedding 3 Large',
    provider: 'OpenAI',
    inputPrice: 0.00013,
    outputPrice: 0,
    contextWindow: 8191,
    features: ['Text embeddings', 'Most capable', 'Multilingual'],
    speed: 'fast',
    category: 'embedding',
    description: 'OpenAI\'s most capable embedding model for both English and non-English tasks.',
    popular: true,
  },
  {
    id: 'openai/text-embedding-ada-002',
    name: 'Text Embedding Ada 002',
    provider: 'OpenAI',
    inputPrice: 0.0001,
    outputPrice: 0,
    contextWindow: 8191,
    features: ['Text embeddings', 'Legacy model', 'Established'],
    speed: 'fast',
    category: 'embedding',
    description: 'OpenAI\'s legacy text embedding model.',
  },

  // Anthropic Models
  {
    id: 'anthropic/claude-opus-4.1',
    name: 'Claude Opus 4.1',
    provider: 'Anthropic',
    inputPrice: 0.015,
    outputPrice: 0.075,
    contextWindow: 200000,
    cacheRead: 0.0015,
    cacheWrite: 0.01875,
    features: ['Superior coding', 'Agentic tasks', 'Drop-in replacement'],
    speed: 'medium',
    category: 'coding',
    description: 'Drop-in replacement for Opus 4 with superior performance for real-world coding and agentic tasks. 74.5% on SWE-bench Verified.',
    popular: true,
  },
  {
    id: 'anthropic/claude-opus-4',
    name: 'Claude Opus 4',
    provider: 'Anthropic',
    inputPrice: 0.015,
    outputPrice: 0.075,
    contextWindow: 200000,
    cacheRead: 0.0015,
    cacheWrite: 0.01875,
    features: ['Best coding', 'Long-running tasks', 'Sustained performance'],
    speed: 'medium',
    category: 'coding',
    description: 'Most powerful model, best coding model in the world. 72.5% on SWE-bench. Delivers sustained performance on long-running tasks requiring thousands of steps.',
    popular: true,
  },
  {
    id: 'anthropic/claude-sonnet-4',
    name: 'Claude Sonnet 4',
    provider: 'Anthropic',
    inputPrice: 0.003,
    outputPrice: 0.015,
    contextWindow: 200000,
    cacheRead: 0.0003,
    cacheWrite: 0.00375,
    features: ['Enhanced coding', 'Steerability', 'Balanced performance'],
    speed: 'medium',
    category: 'coding',
    description: 'Significantly improves on Sonnet 3.7, excelling in coding with 72.7% on SWE-bench. Balances performance and efficiency with enhanced steerability.',
    popular: true,
  },
  {
    id: 'anthropic/claude-3.7-sonnet',
    name: 'Claude 3.7 Sonnet',
    provider: 'Anthropic',
    inputPrice: 0.003,
    outputPrice: 0.015,
    contextWindow: 200000,
    cacheRead: 0.0003,
    cacheWrite: 0.00375,
    features: ['Hybrid reasoning', 'Most intelligent', 'Planning'],
    speed: 'medium',
    category: 'reasoning',
    description: 'First hybrid reasoning model and most intelligent model to date. State-of-the-art for coding, content generation, data analysis, and planning.',
    popular: true,
  },
  {
    id: 'anthropic/claude-3.5-sonnet',
    name: 'Claude 3.5 Sonnet',
    provider: 'Anthropic',
    inputPrice: 0.003,
    outputPrice: 0.015,
    contextWindow: 200000,
    cacheRead: 0.0003,
    cacheWrite: 0.00375,
    features: ['Enterprise workloads', 'High endurance', 'Balanced'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Strikes ideal balance between intelligence and speed for enterprise workloads. Strong performance at lower cost, engineered for high endurance.',
  },
  {
    id: 'anthropic/claude-3.5-haiku',
    name: 'Claude 3.5 Haiku',
    provider: 'Anthropic',
    inputPrice: 0.0008,
    outputPrice: 0.004,
    contextWindow: 200000,
    cacheRead: 0.00008,
    cacheWrite: 0.001,
    features: ['Next generation', 'Fastest', 'Improved skills'],
    speed: 'fast',
    category: 'chat',
    description: 'Next generation fastest model. Similar speed to Claude 3 Haiku but improves across every skill set, surpassing Claude 3 Opus on many benchmarks.',
    popular: true,
  },
  {
    id: 'anthropic/claude-3-haiku',
    name: 'Claude 3 Haiku',
    provider: 'Anthropic',
    inputPrice: 0.00025,
    outputPrice: 0.00125,
    contextWindow: 200000,
    cacheRead: 0.00003,
    cacheWrite: 0.0003,
    features: ['Enterprise workloads', 'Document analysis', 'Cost effective'],
    speed: 'fast',
    category: 'chat',
    description: 'Fastest model designed for enterprise workloads with longer prompts. Quickly analyzes large documents for half the cost of competitors.',
  },
  {
    id: 'anthropic/claude-3-opus',
    name: 'Claude 3 Opus',
    provider: 'Anthropic',
    inputPrice: 0.015,
    outputPrice: 0.075,
    contextWindow: 200000,
    cacheRead: 0.0015,
    cacheWrite: 0.01875,
    features: ['Most intelligent', 'Complex tasks', 'Human-like understanding'],
    speed: 'slow',
    category: 'reasoning',
    description: 'Most intelligent model with best-in-market performance on complex tasks. Navigates open-ended prompts with fluency and human-like understanding.',
  },

  // Google Models
  {
    id: 'google/gemini-2.5-pro',
    name: 'Gemini 2.5 Pro',
    provider: 'Google',
    inputPrice: 0.0025,
    outputPrice: 0.01,
    contextWindow: 1000000,
    features: ['Advanced reasoning', 'Multimodal', 'Massive context'],
    speed: 'medium',
    category: 'multimodal',
    description: 'Most advanced reasoning model capable of solving complex problems. 2M token context window, supports multimodal inputs including text, images, audio, video, and PDFs.',
    popular: true,
  },
  {
    id: 'google/gemini-2.5-flash',
    name: 'Gemini 2.5 Flash',
    provider: 'Google',
    inputPrice: 0.0003,
    outputPrice: 0.0025,
    contextWindow: 1000000,
    features: ['Thinking model', 'Balanced performance', 'Multimodal'],
    speed: 'fast',
    category: 'multimodal',
    description: 'Thinking model with well-rounded capabilities. Balances price and performance with multimodal support and 1M token context window.',
    popular: true,
  },
  {
    id: 'google/gemini-2.5-flash-lite',
    name: 'Gemini 2.5 Flash-Lite',
    provider: 'Google',
    inputPrice: 0.0001,
    outputPrice: 0.0004,
    contextWindow: 1000000,
    features: ['Low latency', 'Configurable thinking', 'Tool connectivity'],
    speed: 'fast',
    category: 'multimodal',
    description: 'Balanced, low-latency model with configurable thinking budgets and tool connectivity. Supports multimodal input with 1M-token context.',
  },
  {
    id: 'google/gemini-2.5-flash-image-preview',
    name: 'Gemini 2.5 Flash Image Preview',
    provider: 'Google',
    inputPrice: 0.0003,
    outputPrice: 0.0025,
    contextWindow: 1000000,
    features: ['Image generation', 'Multi-turn editing', 'Locale-aware'],
    speed: 'medium',
    category: 'multimodal',
    description: 'First fully hybrid reasoning model for image generation. Supports conversational, multi-turn image editing in natural language. Locale-aware for culturally appropriate generation.',
  },
  {
    id: 'google/gemini-2.0-flash',
    name: 'Gemini 2.0 Flash',
    provider: 'Google',
    inputPrice: 0.00015,
    outputPrice: 0.0006,
    contextWindow: 1000000,
    features: ['Next-gen features', 'Built-in tools', 'Multimodal generation'],
    speed: 'fast',
    category: 'multimodal',
    description: 'Next-gen features with superior speed, built-in tool use, multimodal generation, and 1M token context window.',
  },
  {
    id: 'google/gemini-2.0-flash-lite',
    name: 'Gemini 2.0 Flash-Lite',
    provider: 'Google',
    inputPrice: 0.00007,
    outputPrice: 0.0003,
    contextWindow: 1000000,
    features: ['Lightweight', 'Improved capabilities', 'Large context'],
    speed: 'fast',
    category: 'multimodal',
    description: 'Lightweight version of Gemini 2.0 Flash with improved capabilities and 1M token context window.',
  },
  {
    id: 'google/gemini-embedding-001',
    name: 'Gemini Embedding 001',
    provider: 'Google',
    inputPrice: 0.00015,
    outputPrice: 0,
    contextWindow: 2048,
    features: ['State-of-the-art', 'Multilingual', 'Code tasks'],
    speed: 'fast',
    category: 'embedding',
    description: 'State-of-the-art embedding model with excellent performance across English, multilingual and code tasks.',
  },
  {
    id: 'google/text-embedding-005',
    name: 'Text Embedding 005',
    provider: 'Google',
    inputPrice: 0.00003,
    outputPrice: 0,
    contextWindow: 2048,
    features: ['English focused', 'Code optimized', 'Fast'],
    speed: 'fast',
    category: 'embedding',
    description: 'English-focused text embedding model optimized for code and English language tasks.',
  },
  {
    id: 'google/text-multilingual-embedding-002',
    name: 'Text Multilingual Embedding 002',
    provider: 'Google',
    inputPrice: 0.00003,
    outputPrice: 0,
    contextWindow: 2048,
    features: ['Multilingual', 'Cross-lingual', 'Global'],
    speed: 'fast',
    category: 'embedding',
    description: 'Multilingual text embedding model optimized for cross-lingual tasks across many languages.',
  },
  {
    id: 'google/gemma-2-9b',
    name: 'Gemma 2 9B',
    provider: 'Google',
    inputPrice: 0.0002,
    outputPrice: 0.0002,
    contextWindow: 8000,
    features: ['Open source', 'Chat optimized', 'Fast inference'],
    speed: 'fast',
    category: 'chat',
    description: '9 billion parameter open source model by Google fine-tuned for chat. Served by Groq with custom LPU hardware for fast inference.',
  },

  // xAI Models
  {
    id: 'xai/grok-4',
    name: 'Grok 4',
    provider: 'x.ai',
    inputPrice: 0.003,
    outputPrice: 0.015,
    contextWindow: 256000,
    features: ['Flagship model', 'Natural language', 'Math', 'Reasoning'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Latest and greatest flagship model offering unparalleled performance in natural language, math and reasoning - the perfect jack of all trades.',
    popular: true,
  },
  {
    id: 'xai/grok-3',
    name: 'Grok 3',
    provider: 'x.ai',
    inputPrice: 0.003,
    outputPrice: 0.015,
    contextWindow: 131000,
    features: ['Enterprise use cases', 'Data extraction', 'Domain knowledge'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Flagship model excelling at enterprise use cases like data extraction, coding, and text summarization. Deep domain knowledge in finance, healthcare, law, and science.',
  },
  {
    id: 'xai/grok-3-fast',
    name: 'Grok 3 Fast',
    provider: 'x.ai',
    inputPrice: 0.005,
    outputPrice: 0.025,
    contextWindow: 131000,
    features: ['Fast variant', 'Faster infrastructure', 'Low latency'],
    speed: 'fast',
    category: 'reasoning',
    description: 'Fast variant of Grok 3 served on faster infrastructure. Significantly faster response times at higher cost per output token.',
  },
  {
    id: 'xai/grok-3-mini',
    name: 'Grok 3 Mini',
    provider: 'x.ai',
    inputPrice: 0.0003,
    outputPrice: 0.0005,
    contextWindow: 131000,
    features: ['Lightweight', 'Thinking traces', 'Logic-based tasks'],
    speed: 'fast',
    category: 'reasoning',
    description: 'Lightweight model that thinks before responding. Great for simple or logic-based tasks. Raw thinking traces are accessible.',
  },
  {
    id: 'xai/grok-3-mini-fast',
    name: 'Grok 3 Mini Fast',
    provider: 'x.ai',
    inputPrice: 0.0006,
    outputPrice: 0.004,
    contextWindow: 131000,
    features: ['Fast variant', 'Ultra-low latency', 'Premium speed'],
    speed: 'fast',
    category: 'reasoning',
    description: 'Fast variant of Grok 3 Mini with significantly faster response times at higher cost.',
  },
  {
    id: 'xai/grok-2',
    name: 'Grok 2',
    provider: 'x.ai',
    inputPrice: 0.002,
    outputPrice: 0.01,
    contextWindow: 131000,
    features: ['State-of-the-art reasoning', 'Advanced chat', 'Coding'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Frontier language model with state-of-the-art reasoning. Advanced chat, coding, and reasoning capabilities.',
  },
  {
    id: 'xai/grok-2-vision',
    name: 'Grok 2 Vision',
    provider: 'x.ai',
    inputPrice: 0.002,
    outputPrice: 0.01,
    contextWindow: 33000,
    features: ['Vision tasks', 'Visual math', 'Document QA'],
    speed: 'medium',
    category: 'multimodal',
    description: 'Excels in vision-based tasks with state-of-the-art performance in visual math reasoning and document QA. Processes documents, diagrams, charts, screenshots, and photographs.',
  },
  {
    id: 'xai/grok-code-fast-1',
    name: 'Grok Code Fast 1',
    provider: 'x.ai',
    inputPrice: 0.0002,
    outputPrice: 0.0015,
    contextWindow: 256000,
    features: ['Fast agentic coding', 'Large context', 'Coding specialist'],
    speed: 'fast',
    category: 'coding',
    description: 'Latest coding model offering fast agentic coding with a 256K context window.',
  },

  // DeepSeek Models
  {
    id: 'deepseek/deepseek-v3.1',
    name: 'DeepSeek V3.1',
    provider: 'DeepSeek',
    inputPrice: 0.0002,
    outputPrice: 0.0008,
    contextWindow: 164000,
    features: ['Long context extension', 'Enhanced dataset', 'Post-trained'],
    speed: 'fast',
    category: 'reasoning',
    description: 'Post-trained on DeepSeek-V3.1-Base with two-phase long context extension. Expanded dataset with additional long documents.',
    popular: true,
  },
  {
    id: 'deepseek/deepseek-v3',
    name: 'DeepSeek V3',
    provider: 'DeepSeek',
    inputPrice: 0.00077,
    outputPrice: 0.00077,
    contextWindow: 164000,
    features: ['Fast processing', 'General purpose', 'Enhanced reasoning'],
    speed: 'fast',
    category: 'reasoning',
    description: 'Fast general-purpose LLM with enhanced reasoning capabilities.',
  },
  {
    id: 'deepseek/deepseek-v3.1-base',
    name: 'DeepSeek V3.1 Base',
    provider: 'DeepSeek',
    inputPrice: 0.0002,
    outputPrice: 0.0008,
    contextWindow: 128000,
    features: ['Improved base', 'Foundation model', 'Enhanced'],
    speed: 'fast',
    category: 'reasoning',
    description: 'Improved version of the DeepSeek V3 base model.',
  },
  {
    id: 'deepseek/deepseek-v3.1-thinking',
    name: 'DeepSeek V3.1 Thinking',
    provider: 'DeepSeek',
    inputPrice: 0.00056,
    outputPrice: 0.00168,
    contextWindow: 128000,
    cacheRead: 0.00007,
    features: ['Hybrid inference', 'Think/Non-Think modes', 'Agent era'],
    speed: 'fast',
    category: 'reasoning',
    description: 'First step toward agent era with revolutionary hybrid inference. Think and Non-Think modes. Faster reasoning than DeepSeek-R1.',
    popular: true,
  },
  {
    id: 'deepseek/deepseek-r1',
    name: 'DeepSeek R1',
    provider: 'DeepSeek',
    inputPrice: 0.00079,
    outputPrice: 0.004,
    contextWindow: 160000,
    features: ['Chain of Thought', 'Detailed reasoning', 'Step-by-step'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Specialized model using Chain of Thought reasoning. Generates detailed reasoning steps accessible through API.',
    popular: true,
  },
  {
    id: 'deepseek/deepseek-r1-distill-llama-70b',
    name: 'DeepSeek R1 Distill Llama 70B',
    provider: 'DeepSeek',
    inputPrice: 0.00075,
    outputPrice: 0.00099,
    contextWindow: 128000,
    features: ['Reinforcement learning', 'Math', 'Code', 'Complex reasoning'],
    speed: 'medium',
    category: 'reasoning',
    description: 'State-of-the-art reasoning model with reinforcement learning. Strong performance on math, code, and complex reasoning tasks.',
  },

  // Meta Models
  {
    id: 'meta/llama-4-scout',
    name: 'Llama 4 Scout',
    provider: 'Meta',
    inputPrice: 0.00008,
    outputPrice: 0.0003,
    contextWindow: 128000,
    features: ['Multimodal', 'Text and images', 'Visual reasoning'],
    speed: 'fast',
    category: 'multimodal',
    description: 'State-of-the-art 17B instruction-tuned multimodal model. Handles text and image inputs for conversational AI, code generation, and visual reasoning.',
    popular: true,
  },
  {
    id: 'meta/llama-4-maverick',
    name: 'Llama 4 Maverick',
    provider: 'Meta',
    inputPrice: 0.00015,
    outputPrice: 0.0006,
    contextWindow: 1000000,
    features: ['High efficiency', 'Massive context', 'Language processing'],
    speed: 'fast',
    category: 'reasoning',
    description: 'High-efficiency language processing with 1M token context window.',
  },
  {
    id: 'meta/llama-3.3-70b',
    name: 'Llama 3.3 70B',
    provider: 'Meta',
    inputPrice: 0.00072,
    outputPrice: 0.00072,
    contextWindow: 128000,
    features: ['Enhanced reasoning', 'Tool use', 'Multilingual'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Upgraded Llama 3.1 70B with enhanced reasoning, tool use, multilingual abilities, and 128K context window.',
  },
  {
    id: 'meta/llama-3.2-90b',
    name: 'Llama 3.2 90B',
    provider: 'Meta',
    inputPrice: 0.00072,
    outputPrice: 0.00072,
    contextWindow: 128000,
    features: ['Image reasoning', 'Visual recognition', 'Captioning'],
    speed: 'medium',
    category: 'multimodal',
    description: 'Instruction-tuned image reasoning model. Optimized for visual recognition, image reasoning, captioning and answering questions about images.',
  },
  {
    id: 'meta/llama-3.2-11b',
    name: 'Llama 3.2 11B',
    provider: 'Meta',
    inputPrice: 0.00016,
    outputPrice: 0.00016,
    contextWindow: 128000,
    features: ['Image reasoning', 'Visual recognition', 'Compact'],
    speed: 'fast',
    category: 'multimodal',
    description: 'Image reasoning generative model for visual recognition, image reasoning, captioning and answering general questions about images.',
  },
  {
    id: 'meta/llama-3.2-3b',
    name: 'Llama 3.2 3B',
    provider: 'Meta',
    inputPrice: 0.00015,
    outputPrice: 0.00015,
    contextWindow: 128000,
    features: ['On-device', 'Multilingual', 'Text-only'],
    speed: 'fast',
    category: 'chat',
    description: 'Text-only model for on-device use cases like multilingual knowledge retrieval, summarization, and rewriting.',
  },
  {
    id: 'meta/llama-3.2-1b',
    name: 'Llama 3.2 1B',
    provider: 'Meta',
    inputPrice: 0.0001,
    outputPrice: 0.0001,
    contextWindow: 128000,
    features: ['Ultra-compact', 'On-device', 'Multilingual'],
    speed: 'fast',
    category: 'chat',
    description: 'Text-only model supporting on-device multilingual knowledge retrieval, summarization, and rewriting.',
  },
  {
    id: 'meta/llama-3.1-70b',
    name: 'Llama 3.1 70B',
    provider: 'Meta',
    inputPrice: 0.00072,
    outputPrice: 0.00072,
    contextWindow: 128000,
    features: ['Expanded context', 'Multilingual', 'Improved reasoning'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Update to Llama 3 70B with expanded 128K context, multilinguality and improved reasoning capabilities.',
  },
  {
    id: 'meta/llama-3.1-8b',
    name: 'Llama 3.1 8B',
    provider: 'Meta',
    inputPrice: 0.00005,
    outputPrice: 0.00008,
    contextWindow: 128000,
    features: ['Multilingual', 'Tool use', 'Agents', 'Coding assistants'],
    speed: 'fast',
    category: 'coding',
    description: 'Powerful performance in smaller package with improved multilingual support, tool use, and 128K context for agents and coding assistants.',
  },
  {
    id: 'meta/llama-3-70b',
    name: 'Llama 3 70B',
    provider: 'Meta',
    inputPrice: 0.00059,
    outputPrice: 0.00079,
    contextWindow: 8000,
    features: ['Instruction following', 'Open source', 'Groq optimized'],
    speed: 'fast',
    category: 'reasoning',
    description: '70B parameter open source model fine-tuned for instruction following. Served by Groq with custom LPU hardware.',
  },
  {
    id: 'meta/llama-3-8b',
    name: 'Llama 3 8B',
    provider: 'Meta',
    inputPrice: 0.00005,
    outputPrice: 0.00008,
    contextWindow: 8000,
    features: ['Instruction following', 'Open source', 'Groq optimized'],
    speed: 'fast',
    category: 'chat',
    description: '8B parameter open source model fine-tuned for instruction following. Served by Groq with custom LPU hardware.',
  },

  // Mistral Models
  {
    id: 'mistral/mistral-large',
    name: 'Mistral Large',
    provider: 'Mistral',
    inputPrice: 0.002,
    outputPrice: 0.006,
    contextWindow: 32000,
    features: ['Complex tasks', 'Large reasoning', 'Code generation'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Ideal for complex tasks requiring large reasoning capabilities. Synthetic text generation, code generation, RAG, or agents.',
  },
  {
    id: 'mistral/mistral-medium',
    name: 'Mistral Medium',
    provider: 'Mistral',
    inputPrice: 0.0004,
    outputPrice: 0.002,
    contextWindow: 128000,
    features: ['Frontier performance', 'Cost effective', 'Claude comparable'],
    speed: 'fast',
    category: 'reasoning',
    description: 'Delivers frontier performance at order of magnitude less cost. Performs at or above 90% of Claude Sonnet 3.7 on benchmarks.',
    popular: true,
  },
  {
    id: 'mistral/mistral-small',
    name: 'Mistral Small',
    provider: 'Mistral',
    inputPrice: 0.0001,
    outputPrice: 0.0003,
    contextWindow: 32000,
    features: ['Bulk tasks', 'Classification', 'Customer support'],
    speed: 'fast',
    category: 'chat',
    description: 'Ideal for simple bulk tasks like classification, customer support, or text generation. Excellent performance at affordable price.',
  },
  {
    id: 'mistral/mixtral-8x22b-instruct',
    name: 'Mixtral 8x22B Instruct',
    provider: 'Mistral',
    inputPrice: 0.0012,
    outputPrice: 0.0012,
    contextWindow: 66000,
    features: ['Mixture of experts', 'Open source', 'Fireworks optimized'],
    speed: 'medium',
    category: 'reasoning',
    description: '8x22B mixture-of-experts open source model served by Fireworks.',
  },
  {
    id: 'mistral/magistral-small',
    name: 'Magistral Small',
    provider: 'Mistral',
    inputPrice: 0.0005,
    outputPrice: 0.0015,
    contextWindow: 128000,
    features: ['Complex thinking', 'Transparent reasoning', 'Multilingual'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Complex thinking with deep understanding and transparent reasoning. High-fidelity reasoning across numerous languages.',
  },
  {
    id: 'mistral/magistral-medium',
    name: 'Magistral Medium',
    provider: 'Mistral',
    inputPrice: 0.002,
    outputPrice: 0.005,
    contextWindow: 128000,
    features: ['Complex thinking', 'Transparent reasoning', 'Verifiable'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Complex thinking with transparent reasoning you can follow and verify. Excels at maintaining high-fidelity reasoning across languages.',
  },
  {
    id: 'mistral/codestral',
    name: 'Codestral',
    provider: 'Mistral',
    inputPrice: 0.0003,
    outputPrice: 0.0009,
    contextWindow: 256000,
    features: ['80+ languages', 'FIM', 'Code correction', 'Test generation'],
    speed: 'fast',
    category: 'coding',
    description: 'State-of-the-art coding model optimized for low-latency, high-frequency use. Proficient in 80+ languages with FIM, code correction, and test generation.',
    popular: true,
  },
  {
    id: 'mistral/devstral-small',
    name: 'Devstral Small',
    provider: 'Mistral',
    inputPrice: 0.0001,
    outputPrice: 0.0003,
    contextWindow: 128000,
    features: ['Software engineering', 'Codebase exploration', 'Engineering agents'],
    speed: 'fast',
    category: 'coding',
    description: 'Agentic LLM for software engineering built with All Hands AI. Excels at exploring codebases, editing multiple files and powering engineering agents.',
  },
  {
    id: 'mistral/ministral-3b',
    name: 'Ministral 3B',
    provider: 'Mistral',
    inputPrice: 0.00004,
    outputPrice: 0.00004,
    contextWindow: 128000,
    features: ['On-device', 'Smart assistants', 'Local analytics'],
    speed: 'fast',
    category: 'chat',
    description: 'Compact, efficient model for on-device tasks like smart assistants and local analytics with low-latency performance.',
  },
  {
    id: 'mistral/ministral-8b',
    name: 'Ministral 8B',
    provider: 'Mistral',
    inputPrice: 0.0001,
    outputPrice: 0.0001,
    contextWindow: 128000,
    features: ['Memory efficient', 'Complex workflows', 'Edge applications'],
    speed: 'fast',
    category: 'chat',
    description: 'More powerful model with faster, memory-efficient inference for complex workflows and demanding edge applications.',
  },
  {
    id: 'mistral/pixtral-12b',
    name: 'Pixtral 12B',
    provider: 'Mistral',
    inputPrice: 0.00015,
    outputPrice: 0.00015,
    contextWindow: 128000,
    features: ['Image understanding', 'Text and vision', 'Multimodal'],
    speed: 'fast',
    category: 'multimodal',
    description: '12B model with image understanding capabilities in addition to text.',
  },
  {
    id: 'mistral/pixtral-large',
    name: 'Pixtral Large',
    provider: 'Mistral',
    inputPrice: 0.002,
    outputPrice: 0.006,
    contextWindow: 128000,
    features: ['Frontier image understanding', 'Documents', 'Charts'],
    speed: 'medium',
    category: 'multimodal',
    description: 'Second multimodal model with frontier-level image understanding. Understands documents, charts and natural images while maintaining text understanding.',
  },
  {
    id: 'mistral/mistral-embed',
    name: 'Mistral Embed',
    provider: 'Mistral',
    inputPrice: 0.0001,
    outputPrice: 0,
    contextWindow: 8192,
    features: ['General purpose', 'Semantic search', 'RAG'],
    speed: 'fast',
    category: 'embedding',
    description: 'General-purpose text embedding model for semantic search, similarity, clustering, and RAG workflows.',
  },
  {
    id: 'mistral/codestral-embed',
    name: 'Codestral Embed',
    provider: 'Mistral',
    inputPrice: 0.00015,
    outputPrice: 0,
    contextWindow: 8192,
    features: ['Code embeddings', 'Code databases', 'Coding assistants'],
    speed: 'fast',
    category: 'embedding',
    description: 'Code embedding model that can embed code databases and repositories to power coding assistants.',
  },

  // Moonshot AI Models
  {
    id: 'moonshotai/kimi-k2',
    name: 'Kimi K2',
    provider: 'Moonshot AI',
    inputPrice: 0.0005,
    outputPrice: 0.002,
    contextWindow: 131000,
    features: ['Agentic tasks', 'Coding', 'State of the art'],
    speed: 'fast',
    category: 'coding',
    description: 'State of the art language model for agentic and coding tasks.',
  },
  {
    id: 'moonshotai/kimi-k2-0905',
    name: 'Kimi K2 0905',
    provider: 'Moonshot AI',
    inputPrice: 0.0006,
    outputPrice: 0.0025,
    contextWindow: 131000,
    features: ['Agentic tasks', 'Tool calling', '1T parameters'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Strong performance on agentic tasks with tool calling, reasoning, and long context. 1T parameters requiring optimized inference stack.',
  },
  {
    id: 'moonshotai/kimi-k2-turbo',
    name: 'Kimi K2 Turbo',
    provider: 'Moonshot AI',
    inputPrice: 0.0024,
    outputPrice: 0.01,
    contextWindow: 256000,
    features: ['High speed', '60 tokens/sec', 'Large context'],
    speed: 'fast',
    category: 'reasoning',
    description: 'High-speed version with same parameters as K2. Output speed 60 tokens/sec (max 100), 256K context.',
  },

  // Z.ai Models
  {
    id: 'zai/glm-4.5-air',
    name: 'GLM 4.5 Air',
    provider: 'Z.ai',
    inputPrice: 0.0002,
    outputPrice: 0.0011,
    contextWindow: 128000,
    features: ['Agent-oriented', 'MoE', 'Streamlined design'],
    speed: 'fast',
    category: 'reasoning',
    description: 'Agent-oriented MoE model with 106B total parameters, 12B active. More streamlined design than GLM-4.5.',
  },
  {
    id: 'zai/glm-4.5',
    name: 'GLM 4.5',
    provider: 'Z.ai',
    inputPrice: 0.0006,
    outputPrice: 0.0022,
    contextWindow: 128000,
    features: ['Flagship', 'Agent-oriented', 'MoE 355B'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Flagship agent-oriented MoE model with 355B total parameters, 32B active per forward pass.',
  },
  {
    id: 'zai/glm-4.5v',
    name: 'GLM 4.5V',
    provider: 'Z.ai',
    inputPrice: 0.0006,
    outputPrice: 0.0018,
    contextWindow: 66000,
    features: ['Multimodal', 'Vision', 'Thinking techniques'],
    speed: 'fast',
    category: 'multimodal',
    description: 'Built on GLM-4.5-Air base, inherits techniques from GLM-4.1V-Thinking with powerful 106B MoE architecture.',
  },

  // Alibaba Models
  {
    id: 'alibaba/qwen3-coder',
    name: 'Qwen3 Coder 480B',
    provider: 'Alibaba',
    inputPrice: 0.0004,
    outputPrice: 0.0016,
    contextWindow: 131000,
    features: ['Programming specialist', 'Agentic code generation', 'Long context'],
    speed: 'fast',
    category: 'coding',
    description: 'Specialized programming model for ultra-efficient agentic code generation with long context and state-of-the-art performance.',
    popular: true,
  },
  {
    id: 'alibaba/qwen3-max',
    name: 'Qwen3 Max',
    provider: 'Alibaba',
    inputPrice: 0.0012,
    outputPrice: 0.006,
    contextWindow: 262000,
    cacheRead: 0.00024,
    features: ['Instruction following', 'Multilingual', 'Tool use'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Improves instruction following, multilingual ability, and tool use with reduced hallucinations.',
    popular: true,
  },
  {
    id: 'alibaba/qwen-3-235b',
    name: 'Qwen 3 235B',
    provider: 'Alibaba',
    inputPrice: 0.00013,
    outputPrice: 0.0006,
    contextWindow: 262000,
    features: ['Mixture of experts', 'Math', 'Reasoning'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Mixture-of-experts LLM with math and reasoning capabilities.',
  },
  {
    id: 'alibaba/qwen-3-32b',
    name: 'Qwen 3 32B',
    provider: 'Alibaba',
    inputPrice: 0.0001,
    outputPrice: 0.0003,
    contextWindow: 128000,
    features: ['World-class', 'Code generation', 'Tool calling'],
    speed: 'fast',
    category: 'coding',
    description: 'World-class model comparable to DeepSeek R1, outperforming GPT-4.1 and Claude Sonnet 3.7. Excels in code-gen, tool-calling, and reasoning.',
  },
  {
    id: 'alibaba/qwen-3-30b',
    name: 'Qwen 3 30B',
    provider: 'Alibaba',
    inputPrice: 0.00008,
    outputPrice: 0.00029,
    contextWindow: 41000,
    features: ['Latest generation', 'Reasoning', 'Agent capabilities'],
    speed: 'fast',
    category: 'reasoning',
    description: 'Latest generation with groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support.',
  },
  {
    id: 'alibaba/qwen-3-14b',
    name: 'Qwen 3 14B',
    provider: 'Alibaba',
    inputPrice: 0.00006,
    outputPrice: 0.00024,
    contextWindow: 41000,
    features: ['Dense model', 'Reasoning', 'Multilingual'],
    speed: 'fast',
    category: 'reasoning',
    description: 'Dense model with advancements in reasoning, instruction-following, agent capabilities, and multilingual support.',
  },
  {
    id: 'alibaba/qwen3-next-80b-a3b-instruct',
    name: 'Qwen3 Next 80B A3B Instruct',
    provider: 'Alibaba',
    inputPrice: 0.00015,
    outputPrice: 0.0015,
    contextWindow: 131000,
    features: ['Non-thinking', 'Chinese understanding', 'Logical reasoning'],
    speed: 'fast',
    category: 'reasoning',
    description: 'New generation open-source non-thinking model. Superior Chinese understanding, augmented logical reasoning, enhanced text generation.',
  },
  {
    id: 'alibaba/qwen3-next-80b-a3b-thinking',
    name: 'Qwen3 Next 80B A3B Thinking',
    provider: 'Alibaba',
    inputPrice: 0.00014,
    outputPrice: 0.0014,
    contextWindow: 164000,
    features: ['Next generation', 'Innovative architecture', 'Agentic AI'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Next-generation foundation model addressing scaling efficiency through innovative architecture for powerful and agentic AI.',
  },

  // Perplexity Models
  {
    id: 'perplexity/sonar',
    name: 'Sonar',
    provider: 'Perplexity',
    inputPrice: 0.001,
    outputPrice: 0.001,
    contextWindow: 127000,
    features: ['Search grounding', 'Lightweight', 'Quick'],
    speed: 'fast',
    category: 'chat',
    description: 'Lightweight offering with search grounding, quicker and cheaper than Sonar Pro.',
  },
  {
    id: 'perplexity/sonar-pro',
    name: 'Sonar Pro',
    provider: 'Perplexity',
    inputPrice: 0.003,
    outputPrice: 0.015,
    contextWindow: 200000,
    features: ['Premier offering', 'Search grounding', 'Advanced queries'],
    speed: 'medium',
    category: 'chat',
    description: 'Premier offering with search grounding, supporting advanced queries and follow-ups.',
    popular: true,
  },
  {
    id: 'perplexity/sonar-reasoning',
    name: 'Sonar Reasoning',
    provider: 'Perplexity',
    inputPrice: 0.001,
    outputPrice: 0.005,
    contextWindow: 127000,
    features: ['Chain of Thought', 'Search grounding', 'Reasoning focused'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Reasoning-focused model that outputs Chain of Thought in responses with search grounding.',
  },
  {
    id: 'perplexity/sonar-reasoning-pro',
    name: 'Sonar Reasoning Pro',
    provider: 'Perplexity',
    inputPrice: 0.002,
    outputPrice: 0.008,
    contextWindow: 127000,
    features: ['Premium reasoning', 'Comprehensive explanations', 'Enhanced search'],
    speed: 'medium',
    category: 'reasoning',
    description: 'Premium reasoning model with Chain of Thought, comprehensive explanations, enhanced search, and multiple search queries per request.',
  },

  // Cohere Models
  {
    id: 'cohere/command-r-plus',
    name: 'Command R+',
    provider: 'Cohere',
    inputPrice: 0.0025,
    outputPrice: 0.01,
    contextWindow: 128000,
    features: ['Conversational', 'Long context', 'Production ready'],
    speed: 'medium',
    category: 'chat',
    description: 'Newest LLM optimized for conversational interaction and long-context tasks. Extremely performant for moving beyond proof of concept to production.',
  },
  {
    id: 'cohere/command-r',
    name: 'Command R',
    provider: 'Cohere',
    inputPrice: 0.00015,
    outputPrice: 0.0006,
    contextWindow: 128000,
    features: ['Conversational', 'Long context', 'Balanced performance'],
    speed: 'fast',
    category: 'chat',
    description: 'Optimized for conversational interaction and long context tasks. Balances high performance with strong accuracy for production.',
  },
  {
    id: 'cohere/command-a',
    name: 'Command A',
    provider: 'Cohere',
    inputPrice: 0.0025,
    outputPrice: 0.01,
    contextWindow: 256000,
    features: ['Most performant', 'Tool use', 'Agents', 'RAG', 'Multilingual'],
    speed: 'fast',
    category: 'reasoning',
    description: 'Most performant model excelling at tool use, agents, RAG, and multilingual cases. 256K context, requires only two GPUs, 150% higher throughput than R+.',
    popular: true,
  },
  {
    id: 'cohere/embed-v4.0',
    name: 'Embed V4.0',
    provider: 'Cohere',
    inputPrice: 0.00012,
    outputPrice: 0,
    contextWindow: 8192,
    features: ['Mixed content', 'Text and images', 'Classification'],
    speed: 'fast',
    category: 'embedding',
    description: 'Allows text, images, or mixed content to be classified or turned into embeddings.',
  },

  // Amazon Models
  {
    id: 'amazon/nova-pro',
    name: 'Nova Pro',
    provider: 'Amazon',
    inputPrice: 0.0008,
    outputPrice: 0.0032,
    contextWindow: 300000,
    features: ['Multimodal', 'Best combination', 'Accuracy and speed'],
    speed: 'fast',
    category: 'multimodal',
    description: 'Highly capable multimodal model with best combination of accuracy, speed, and cost for wide range of tasks.',
    popular: true,
  },
  {
    id: 'amazon/nova-lite',
    name: 'Nova Lite',
    provider: 'Amazon',
    inputPrice: 0.00006,
    outputPrice: 0.00024,
    contextWindow: 300000,
    features: ['Very low cost', 'Lightning fast', 'Multimodal'],
    speed: 'fast',
    category: 'multimodal',
    description: 'Very low cost multimodal model that is lightning fast for processing image, video, and text inputs.',
  },
  {
    id: 'amazon/nova-micro',
    name: 'Nova Micro',
    provider: 'Amazon',
    inputPrice: 0.00004,
    outputPrice: 0.00014,
    contextWindow: 128000,
    features: ['Text-only', 'Lowest latency', 'Very low cost'],
    speed: 'fast',
    category: 'chat',
    description: 'Text-only model that delivers the lowest latency responses at very low cost.',
  },
  {
    id: 'amazon/titan-embed-text-v2',
    name: 'Titan Embed Text V2',
    provider: 'Amazon',
    inputPrice: 0.00002,
    outputPrice: 0,
    contextWindow: 8192,
    features: ['Lightweight', 'Efficient', 'Multilingual', 'Multiple dimensions'],
    speed: 'fast',
    category: 'embedding',
    description: 'Light weight, efficient multilingual embedding model supporting 1024, 512, and 256 dimensions.',
  },

  // Vercel Models
  {
    id: 'vercel/v0-1.5-md',
    name: 'V0 1.5 MD',
    provider: 'Vercel',
    inputPrice: 0.003,
    outputPrice: 0.015,
    contextWindow: 128000,
    features: ['Web app generation', 'Framework-specific', 'Modern web'],
    speed: 'medium',
    category: 'coding',
    description: 'Access the model behind v0 to generate, fix, and optimize modern web apps with framework-specific reasoning and up-to-date knowledge.',
    popular: true,
  },
  {
    id: 'vercel/v0-1.0-md',
    name: 'V0 1.0 MD',
    provider: 'Vercel',
    inputPrice: 0.003,
    outputPrice: 0.015,
    contextWindow: 128000,
    features: ['Web app generation', 'Framework-specific', 'Modern web'],
    speed: 'medium',
    category: 'coding',
    description: 'Access the model behind v0 to generate, fix, and optimize modern web apps with framework-specific reasoning and up-to-date knowledge.',
  },

  // Voyage Models
  {
    id: 'voyage/voyage-3.5-lite',
    name: 'Voyage 3.5 Lite',
    provider: 'Voyage',
    inputPrice: 0.00002,
    outputPrice: 0,
    contextWindow: 16384,
    features: ['Latency optimized', 'Cost optimized', 'Embeddings'],
    speed: 'fast',
    category: 'embedding',
    description: 'Embedding model optimized for latency and cost.',
  },
  {
    id: 'voyage/voyage-3.5',
    name: 'Voyage 3.5',
    provider: 'Voyage',
    inputPrice: 0.00006,
    outputPrice: 0,
    contextWindow: 16384,
    features: ['General purpose', 'Multilingual', 'Retrieval quality'],
    speed: 'fast',
    category: 'embedding',
    description: 'Embedding model optimized for general-purpose and multilingual retrieval quality.',
  },
  {
    id: 'voyage/voyage-3-large',
    name: 'Voyage 3 Large',
    provider: 'Voyage',
    inputPrice: 0.00018,
    outputPrice: 0,
    contextWindow: 16384,
    features: ['Best quality', 'General purpose', 'Multilingual'],
    speed: 'fast',
    category: 'embedding',
    description: 'Embedding model with the best general-purpose and multilingual retrieval quality.',
    popular: true,
  },
  {
    id: 'voyage/voyage-code-3',
    name: 'Voyage Code 3',
    provider: 'Voyage',
    inputPrice: 0.00018,
    outputPrice: 0,
    contextWindow: 16384,
    features: ['Code retrieval', 'Programming optimized', 'Latest'],
    speed: 'fast',
    category: 'embedding',
    description: 'Embedding model optimized for code retrieval.',
  },
  {
    id: 'voyage/voyage-code-2',
    name: 'Voyage Code 2',
    provider: 'Voyage',
    inputPrice: 0.00012,
    outputPrice: 0,
    contextWindow: 16384,
    features: ['Code embeddings', '17% better', 'Previous generation'],
    speed: 'fast',
    category: 'embedding',
    description: 'Previous generation code embeddings model, 17% better than alternatives.',
  },
  {
    id: 'voyage/voyage-finance-2',
    name: 'Voyage Finance 2',
    provider: 'Voyage',
    inputPrice: 0.00012,
    outputPrice: 0,
    contextWindow: 16384,
    features: ['Finance optimized', 'RAG', 'Domain specific'],
    speed: 'fast',
    category: 'embedding',
    description: 'Embedding model optimized for finance retrieval and RAG.',
  },
  {
    id: 'voyage/voyage-law-2',
    name: 'Voyage Law 2',
    provider: 'Voyage',
    inputPrice: 0.00012,
    outputPrice: 0,
    contextWindow: 16384,
    features: ['Legal optimized', 'RAG', 'Domain specific'],
    speed: 'fast',
    category: 'embedding',
    description: 'Embedding model optimized for legal retrieval and RAG.',
  },

  // Other Models
  {
    id: 'stealth/sonoma-sky-alpha',
    name: 'Sonoma Sky Alpha',
    provider: 'Stealth',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 2000000,
    features: ['Maximally intelligent', 'Frontier model', 'Image inputs', 'Parallel tools'],
    speed: 'medium',
    category: 'multimodal',
    description: 'Maximally intelligent general-purpose frontier model with 2M token context. Supports image inputs and parallel tool calling. Note: prompts/responses may be retained for training during stealth period.',
    popular: true,
  },
  {
    id: 'stealth/sonoma-dusk-alpha',
    name: 'Sonoma Dusk Alpha',
    provider: 'Stealth',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 2000000,
    features: ['Fast and intelligent', 'Frontier model', 'Image inputs', 'Parallel tools'],
    speed: 'fast',
    category: 'multimodal',
    description: 'Fast and intelligent general-purpose frontier model with 2M token context. Supports image inputs and parallel tool calling. Note: prompts/responses may be retained for training during stealth period.',
  },
  {
    id: 'morph/morph-v3-fast',
    name: 'Morph V3 Fast',
    provider: 'Morph',
    inputPrice: 0.0008,
    outputPrice: 0.0012,
    contextWindow: 82000,
    features: ['Code changes', '4500+ tokens/sec', 'AI coding workflow'],
    speed: 'fast',
    category: 'coding',
    description: 'Specialized AI model that applies code changes suggested by frontier models to existing code files FAST - 4500+ tokens/second. Final step in AI coding workflow.',
  },
  {
    id: 'morph/morph-v3-large',
    name: 'Morph V3 Large',
    provider: 'Morph',
    inputPrice: 0.0009,
    outputPrice: 0.0019,
    contextWindow: 82000,
    features: ['Code changes', '2500+ tokens/sec', 'AI coding workflow'],
    speed: 'fast',
    category: 'coding',
    description: 'Specialized AI model that applies code changes suggested by frontier models to existing code files FAST - 2500+ tokens/second. Final step in AI coding workflow.',
  },
  {
    id: 'meituan/longcat-flash-chat',
    name: 'LongCat Flash Chat',
    provider: 'Meituan',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 128000,
    features: ['High throughput', 'MoE', 'Agentic tasks'],
    speed: 'fast',
    category: 'chat',
    description: 'High-throughput MoE chat model with 128K context optimized for agentic tasks.',
  },
  {
    id: 'inception/mercury-coder-small',
    name: 'Mercury Coder Small',
    provider: 'Inception',
    inputPrice: 0.00025,
    outputPrice: 0.001,
    contextWindow: 32000,
    features: ['Code generation', 'Debugging', 'Refactoring', 'Minimal latency'],
    speed: 'fast',
    category: 'coding',
    description: 'Ideal for code generation, debugging, and refactoring tasks with minimal latency.',
  },
];

export const providers = Array.from(new Set(allModels.map(m => m.provider))).sort();

export const categories = Array.from(new Set(allModels.map(m => m.category))).sort();

export function getModelsByProvider(provider: string) {
  return allModels.filter(m => m.provider === provider);
}

export function getModelsByCategory(category: string) {
  return allModels.filter(m => m.category === category);
}

export function getPopularModels() {
  return allModels.filter(m => m.popular);
}

export function searchModels(query: string) {
  const lowQuery = query.toLowerCase();
  return allModels.filter(m =>
    m.name.toLowerCase().includes(lowQuery) ||
    m.provider.toLowerCase().includes(lowQuery) ||
    m.features.some(f => f.toLowerCase().includes(lowQuery)) ||
    m.description?.toLowerCase().includes(lowQuery)
  );
}

export function getModelById(id: string) {
  return allModels.find(m => m.id === id);
}